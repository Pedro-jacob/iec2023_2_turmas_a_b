{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "19RmkVaaqgF8BielSaGduMjwXmPea3Uq8",
      "authorship_tag": "ABX9TyNG1WlxaCL4t+pN/x5PfSzn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pedro-jacob/iec2023_2_turmas_a_b/blob/main/entregas-Pedro%20campos%20jacob/modelo_de_IA_IEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7-oVWLnmuu-",
        "outputId": "3208cc88-ff5d-4d3c-e9e0-c021da328854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q tensorflow==2.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "jy4-JjkuuOQt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o dataset CIFAR-10 - Já está no Keras!\n",
        "(imagens_treino, labels_treino), (imagens_teste, labels_teste) = datasets.cifar10.load_data()"
      ],
      "metadata": {
        "id": "VfP2GWM2vNGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes das imagens\n",
        "nomes_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "metadata": {
        "id": "IdIwL4oawRk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normaliza os valores dos pixels para que os dados fiquem na mesma escala\n",
        "imagens_treino = imagens_treino / 255.0\n",
        "imagens_teste = imagens_teste / 255.0"
      ],
      "metadata": {
        "id": "IOBZEpVgw3KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para exibir as imagens\n",
        "def visualiza_imagens(images, labels):\n",
        "    plt.figure(figsize = (10,10))\n",
        "    for i in range(25):\n",
        "        plt.subplot(5, 5, i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(images[i], cmap = plt.cm.binary)\n",
        "        plt.xlabel(nomes_classes[labels[i][0]])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Q3YdG1ABxGQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executa a função\n",
        "visualiza_imagens(imagens_treino, labels_treino)"
      ],
      "metadata": {
        "id": "qi7wx_mSxXcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arquitetura do Modelo -> Feature Learning\n",
        "\n",
        "# Cria o objeto de sequência de camadas - do Keras\n",
        "modelo_lia = models.Sequential()\n",
        "\n",
        "# Adiciona o primeiro bloco de convolução e max pooling (camada de entrada)\n",
        "modelo_lia.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (32, 32, 3)))\n",
        "modelo_lia.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Adiciona o segundo bloco de convolução e max pooling (camada intermediária)\n",
        "modelo_lia.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
        "modelo_lia.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Adiciona o terceiro bloco de convolução e max pooling (camada intermediária)\n",
        "modelo_lia.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\n",
        "modelo_lia.add(layers.MaxPooling2D((2, 2)))"
      ],
      "metadata": {
        "id": "IiLoo3gTzn3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar camadas de classificação - Converte para um vetor - flatten\n",
        "modelo_lia.add(layers.Flatten())\n",
        "modelo_lia.add(layers.Dense(64, activation = 'relu'))\n",
        "modelo_lia.add(layers.Dense(10, activation = 'softmax'))"
      ],
      "metadata": {
        "id": "6U9s0CQa0ECS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilação do modelo\n",
        "modelo_lia.compile(optimizer = 'adam',\n",
        "                   loss = 'sparse_categorical_crossentropy',\n",
        "                   metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "_szB-kKb0W7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = modelo_lia.fit(imagens_treino,\n",
        "                         labels_treino,\n",
        "                         epochs = 10,\n",
        "                         validation_data = (imagens_teste, labels_teste))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcHtDL2k1c8A",
        "outputId": "9ca0d411-c6aa-4b99-fd83-411fdbc95e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 88s 55ms/step - loss: 1.5894 - accuracy: 0.4164 - val_loss: 1.4539 - val_accuracy: 0.4853\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 88s 56ms/step - loss: 1.2520 - accuracy: 0.5533 - val_loss: 1.2116 - val_accuracy: 0.5699\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 1.1148 - accuracy: 0.6037 - val_loss: 1.1014 - val_accuracy: 0.6077\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 85s 54ms/step - loss: 1.0187 - accuracy: 0.6440 - val_loss: 1.0084 - val_accuracy: 0.6475\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.9562 - accuracy: 0.6645 - val_loss: 1.0465 - val_accuracy: 0.6306\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.9014 - accuracy: 0.6826 - val_loss: 0.9879 - val_accuracy: 0.6570\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 87s 56ms/step - loss: 0.8593 - accuracy: 0.6978 - val_loss: 0.9558 - val_accuracy: 0.6730\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.8204 - accuracy: 0.7128 - val_loss: 0.9254 - val_accuracy: 0.6842\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 87s 56ms/step - loss: 0.7841 - accuracy: 0.7250 - val_loss: 0.9256 - val_accuracy: 0.6889\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.7609 - accuracy: 0.7327 - val_loss: 0.9348 - val_accuracy: 0.6890\n",
            "CPU times: user 21min 10s, sys: 19.1 s, total: 21min 29s\n",
            "Wall time: 14min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Avalia o modelo\n",
        "erro_teste, acc_teste = modelo_lia.evaluate(imagens_teste, labels_teste, verbose = 2)\n",
        "\n",
        "print('\\nAcurácia com dados de Teste:', acc_teste)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN7rFFkX4z96",
        "outputId": "1b17cef5-dd66-45bb-f06f-07aedfcb2e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 4s - loss: 0.9348 - accuracy: 0.6890 - 4s/epoch - 13ms/step\n",
            "\n",
            "Acurácia com dados de Teste: 0.6890000104904175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega uma nova imagem\n",
        "nova_imagem = Image.open(\"https://t.ctcdn.com.br/XBjoqig9COaA6GJXQ3xE8dB_Kpw=/640x360/smart/i321185.jpeg\")\n",
        "\n",
        "# Redimensiona para 32x32 pixels\n",
        "nova_imagem = nova_imagem.resize((32, 32))\n",
        "\n",
        "# Converte a imagem para um array NumPy e normaliza\n",
        "nova_imagem_array = np.array(nova_imagem) / 255.0\n",
        "\n",
        "# Expande a dimensão do array para que ele tenha o formato (1, 32, 32, 3)\n",
        "nova_imagem_array = np.expand_dims(nova_imagem_array, axis = 0)\n",
        "\n",
        "# Previsões\n",
        "previsoes = modelo_lia.predict(nova_imagem_array)\n",
        "\n",
        "print(previsoes)\n",
        "\n",
        "# Obtém a classe com maior probabilidade e o nome da classe\n",
        "classe_prevista = np.argmax(previsoes)\n",
        "nome_classe_prevista = nomes_classes[classe_prevista]\n",
        "\n",
        "print(\"A nova imagem foi classificada como:\", nome_classe_prevista)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "Ys3hTHKR5bvS",
        "outputId": "4b76eb86-91b9-432f-db75-7acc1908f40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'https://classic.exame.com/wp-content/uploads/2023/08/cachorro1.jpg?quality=70&strip=info&w=1024'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-81e264e06e75>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Carrega uma nova imagem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnova_imagem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://classic.exame.com/wp-content/uploads/2023/08/cachorro1.jpg?quality=70&strip=info&w=1024\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Redimensiona para 32x32 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnova_imagem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnova_imagem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://classic.exame.com/wp-content/uploads/2023/08/cachorro1.jpg?quality=70&strip=info&w=1024'"
          ]
        }
      ]
    }
  ]
}